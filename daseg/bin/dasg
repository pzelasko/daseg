#!/usr/bin/env python
import pickle
import sys
from pathlib import Path
from typing import Optional, Dict, List

import click
import pytorch_lightning as pl
import torch
from pytorch_lightning.callbacks import ModelCheckpoint
from torch.utils.data import Dataset
from torch.utils.data.dataloader import DataLoader

from daseg import DialogActCorpus, TransformerModel
from daseg.data import BLANK
from daseg.dataloaders.transformers import to_dataset, to_dataloader
from daseg.dataloaders.turns import SingleTurnDataset, padding_collate_fn
from daseg.models.bigru import ZhaoKawaharaBiGru
from daseg.models.transformer_pl import DialogActTransformer
from daseg.slack import SlackNotifier

if torch.cuda.is_available():
    torch.tensor([0], device='cuda')


@click.group()
def cli():
    pass


@cli.command()
@click.argument('output_dir', type=click.Path())
@click.option('--dataset-path', default='deps/swda/swda', type=click.Path(exists=True))
@click.option('--window-size', type=int, default=None)
@click.option('--window-overlap', type=int, default=None)
@click.option('-p', '--strip-punctuation-and-lowercase', is_flag=True)
@click.option('-c', '--continuations-allowed', is_flag=True)
@click.option('-m', '--merge-continuations', is_flag=True)
@click.option('-t', '--window-test', is_flag=True)
@click.option('-s', '--tagset', default='basic',
              type=click.Choice(['basic', 'general', 'full', 'broken_swda', 'segmentation']))
@click.option('-j', '--no-joint-coding', is_flag=True)
@click.option('-n', '--turns', is_flag=True)
def prepare_data(
        output_dir: str,
        dataset_path: str,
        window_size: Optional[int],
        window_overlap: Optional[int],
        strip_punctuation_and_lowercase: bool,
        continuations_allowed: bool,
        merge_continuations: bool,
        window_test: bool,
        tagset: str,
        no_joint_coding: bool,
        turns: bool
):
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    dataset = DialogActCorpus.from_path(
        dataset_path=dataset_path,
        strip_punctuation_and_lowercase=strip_punctuation_and_lowercase,
        tagset=tagset,
        merge_continuations=merge_continuations
    )
    for split_name, split_dataset in dataset.train_dev_test_split().items():
        split_dataset.dump_for_transformers_ner(
            output_dir / f'{split_name}.txt.tmp',
            acts_count_per_sample=window_size if split_name != 'test' or window_test else None,
            acts_count_overlap=window_overlap if split_name != 'test' or window_test else None,
            continuations_allowed=continuations_allowed,
            use_joint_coding=not no_joint_coding,
            use_turns=turns,
        )


@cli.command()
@click.argument('model_path', type=click.Path(exists=True))
@click.option('--dataset-path', default='deps/swda/swda', type=click.Path(exists=True))
@click.option('--split', type=click.Choice(['train', 'dev', 'test']), default='test')
@click.option('-b', '--batch_size', default=4, type=int)
@click.option('-l', '--window_len', default=None, type=int)
@click.option('--device', default='cpu', type=str)
@click.option('-o', '--save-output', default=None, type=click.Path())
@click.option('-p', '--strip-punctuation-and-lowercase', is_flag=True)
@click.option('-m', '--merge-continuations', is_flag=True)
@click.option('-r', '--begin-determines-act', is_flag=True)
@click.option('-s', '--tagset', default='basic',
              type=click.Choice(['basic', 'general', 'full', 'broken_swda', 'segmentation']))
@click.option('-d', '--dont-propagate-context', is_flag=True)
@click.option('-v', '--verbose', is_flag=True)
@click.option('-j', '--no-joint-coding', is_flag=True)
@click.option('-n', '--turns', is_flag=True)
def evaluate(
        model_path: str,
        dataset_path: str,
        split: str,
        batch_size: int,
        window_len: int,
        device: str,
        save_output: Optional[str],
        strip_punctuation_and_lowercase: bool,
        merge_continuations: bool,
        begin_determines_act: bool,
        tagset: str,
        dont_propagate_context: bool,
        verbose: bool,
        no_joint_coding: bool,
        turns: bool,
):
    dataset = DialogActCorpus.from_path(
        dataset_path,
        splits=[split],
        strip_punctuation_and_lowercase=strip_punctuation_and_lowercase,
        tagset=tagset,
        merge_continuations=merge_continuations
    )
    model = TransformerModel.from_path(Path(model_path), device=device)
    results = model.predict(
        dataset=dataset,
        batch_size=batch_size,
        window_len=window_len,
        propagate_context=not dont_propagate_context,
        begin_determines_act=begin_determines_act,
        verbose=verbose,
        use_joint_coding=not no_joint_coding,
        use_turns=turns
    )
    with SlackNotifier(' '.join(sys.argv)) as slack:
        for res_grp in (
                'sklearn_metrics', 'seqeval_metrics', 'zhao_kawahara_metrics', 'ORIGINAL_zhao_kawahara_metrics'):
            slack.write_and_print(f'{res_grp.upper()}:')
            for key, val in results[res_grp].items():
                slack.write_and_print(f'{key}\t{val:.2%}')
    if save_output is not None:
        with open(save_output, 'wb') as f:
            pickle.dump(results, f)


@cli.command()
@click.argument('output-dir', type=click.Path())
@click.option('-o', '--model-name-or-path', default='allenai/longformer-base-4096')
@click.option('--dataset-path', default='deps/swda/swda', type=click.Path(exists=True))
@click.option('-p', '--strip-punctuation-and-lowercase', is_flag=True)
@click.option('-m', '--merge-continuations', is_flag=True)
@click.option('-s', '--tagset', default='basic',
              type=click.Choice(['basic', 'general', 'full', 'broken_swda', 'segmentation']))
@click.option('-l', '--max-sequence-length', default=4096, type=int)
@click.option('-n', '--turns', is_flag=True)
@click.option('-w', '--windows-if-exceeds-max-len', is_flag=True)
def prepare_exp(
        output_dir: Path,
        model_name_or_path: str,
        dataset_path: str,
        strip_punctuation_and_lowercase: bool,
        merge_continuations: bool,
        tagset: str,
        max_sequence_length: int,
        turns: bool,
        windows_if_exceeds_max_len: bool,
):
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    corpus = DialogActCorpus.from_path(
        dataset_path,
        strip_punctuation_and_lowercase=strip_punctuation_and_lowercase,
        tagset=tagset,
        merge_continuations=merge_continuations
    )
    model = DialogActTransformer(
        labels=corpus.joint_coding_dialog_act_labels,
        model_name_or_path=model_name_or_path
    )
    dsets = {
        key: to_dataset(
            corpus=split_corpus,
            tokenizer=model.tokenizer,
            model_type=model.config.model_type,
            labels=model.labels,
            max_seq_length=max_sequence_length,
            use_turns=turns,
            windows_if_exceeds_max_length=windows_if_exceeds_max_len
        )
        for key, split_corpus in corpus.train_dev_test_split().items()
    }
    with open(output_path / 'dataset.pkl', 'wb') as f:
        pickle.dump(dsets, f)
    with open(output_path / 'labels.pkl', 'wb') as f:
        pickle.dump(corpus.joint_coding_dialog_act_labels, f)


@cli.command()
@click.argument('exp-dir', type=click.Path())
@click.option('-o', '--model-name-or-path', default='allenai/longformer-base-4096')
@click.option('-b', '--batch-size', default=1, type=int)
@click.option('-c', '--val-batch-size', default=8, type=int)
@click.option('-e', '--epochs', default=10, type=int)
@click.option('-a', '--gradient-accumulation-steps', default=1, type=int)
@click.option('-r', '--random-seed', default=1050, type=int)
@click.option('-g', '--num-gpus', default=0, type=int)
@click.option('-f', '--fp16', is_flag=True)
def train_transformer(
        exp_dir: Path,
        model_name_or_path: str,
        batch_size: int,
        val_batch_size: int,
        epochs: int,
        gradient_accumulation_steps: int,
        random_seed: int,
        num_gpus: int,
        fp16: bool
):
    pl.seed_everything(random_seed)
    output_path = Path(exp_dir)
    with open(output_path / 'dataset.pkl', 'rb') as f:
        datasets: Dict[str, Dataset] = pickle.load(f)
    with open(output_path / 'labels.pkl', 'rb') as f:
        labels: List[str] = pickle.load(f)
    model = DialogActTransformer(labels=labels, model_name_or_path=model_name_or_path)
    loaders = {
        key: to_dataloader(dset, batch_size=batch_size if key == 'train' else val_batch_size)
        for key, dset in datasets.items()
    }
    model.compute_and_set_total_steps(
        dataloader=loaders['train'],
        gradient_accumulation_steps=gradient_accumulation_steps,
        num_epochs=epochs
    )
    model.set_output_dir(exp_dir)
    trainer = pl.Trainer(
        gradient_clip_val=1.0,
        default_root_dir=str(exp_dir),
        gpus=num_gpus,
        deterministic=True,
        checkpoint_callback=ModelCheckpoint(
            filepath=str(exp_dir),
            prefix='checkpoint',
            verbose=True,
            monitor='macro_f1',
            mode='min',
            save_top_k=True
        ),
        max_epochs=epochs,
        accumulate_grad_batches=gradient_accumulation_steps,
        precision=16 if fp16 else 32,
    )
    trainer.fit(
        model=model,
        train_dataloader=loaders['train'],
        val_dataloaders=loaders['dev']
    )
    trainer.test(
        model=model,
        test_dataloaders=loaders['test']
    )


@cli.command()
@click.argument('output-dir', type=click.Path())
@click.option('--dataset-path', default='deps/swda/swda', type=click.Path(exists=True))
@click.option('-b', '--batch-size', default=30, type=int)
@click.option('-e', '--epochs', default=10, type=int)
@click.option('-r', '--random-seed', default=1050, type=int)
@click.option('-p', '--strip-punctuation-and-lowercase', is_flag=True)
@click.option('-m', '--merge-continuations', is_flag=True)
@click.option('-s', '--tagset', default='basic',
              type=click.Choice(['basic', 'general', 'full', 'broken_swda', 'segmentation']))
@click.option('-g', '--num-gpus', default=0, type=int)
@click.option('-f', '--frequency-weighted', is_flag=True)
@click.option('-o', '--save-output', default=None, type=click.Path())
def train_bigru(
        output_dir: str,
        dataset_path: str,
        batch_size: int,
        epochs: int,
        random_seed: int,
        strip_punctuation_and_lowercase: bool,
        merge_continuations: bool,
        tagset: str,
        num_gpus: int,
        frequency_weighted: bool,
        save_output: str
):
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    pl.seed_everything(random_seed)
    corpus = DialogActCorpus.from_path(
        dataset_path,
        strip_punctuation_and_lowercase=strip_punctuation_and_lowercase,
        tagset=tagset,
        merge_continuations=merge_continuations
    ).with_limited_vocabulary(10000)
    word2idx = {w: i + 1 for i, w in enumerate(corpus.vocabulary.keys())}
    tag2idx = {t: i for i, t in enumerate(t for t in corpus.joint_coding_dialog_act_labels if t != BLANK)}
    with open(output_path / 'word2idx', 'w') as f:
        for k, v in word2idx.items(): print(f'{k} {v}', file=f)
    with open(output_path / 'tag2idx', 'w') as f:
        for k, v in tag2idx.items(): print(f'{k} {v}', file=f)
    loaders = {
        key: DataLoader(
            dataset=SingleTurnDataset(split_corpus, word2idx=word2idx, tag2idx=tag2idx),
            batch_size=batch_size,
            shuffle=key == 'train',
            collate_fn=padding_collate_fn,
            num_workers=4,
            pin_memory=True
        )
        for key, split_corpus in corpus.train_dev_test_split().items()
    }
    model = ZhaoKawaharaBiGru(
        vocab=word2idx,
        labels=tag2idx,
        weight_drop=0.5,
        label_frequencies=loaders[
            'train'].dataset.corpus.joint_coding_dialog_act_label_frequencies if frequency_weighted else None
    )
    trainer = pl.Trainer(
        gradient_clip_val=0.5,  # note: not necessarily the right method of gradient clipping
        default_root_dir=output_dir,
        gpus=num_gpus,
        deterministic=True,
        checkpoint_callback=ModelCheckpoint(
            filepath=output_dir,
            verbose=True,
            monitor='val_loss',
            mode='min'
        ),
        max_epochs=epochs
    )
    trainer.fit(
        model=model,
        train_dataloader=loaders['train'],
        val_dataloaders=loaders['dev']
    )
    trainer.test(
        model=model,
        test_dataloaders=loaders['test']
    )
    model.save_results(Path(save_output))


if __name__ == '__main__':
    cli()
